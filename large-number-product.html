<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Optimizing the Computation Time of the Product of Two "Large" Numbers</title>
    <style>
        body {
            background-color: #050606ea;
            margin: 0;
            padding: 0;
            min-height: 100vh;
            color: #fff;
            font-family: 'Segoe UI', Arial, sans-serif;
            line-height: 1.6;
        }
        .container {
            margin: 40px auto;
            max-width: 900px;
            padding: 32px;
            background: #181a1b;
            border-radius: 12px;
            box-shadow: 0 4px 20px #000a;
        }
        h1 {
            color: #7ecfff;
            font-size: 2.2em;
            margin-top: 0;
            border-bottom: 1px solid #333;
            padding-bottom: 10px;
        }
        h2 {
            color: #7ecfff;
            margin-top: 40px;
            font-size: 1.5em;
        }
        h3 {
            color: #a8ff78;
            margin-top: 30px;
            font-size: 1.2em;
        }
        p {
            margin: 15px 0;
            text-align: justify;
        }
        .abstract {
            background: #1a2633;
            padding: 20px;
            border-left: 5px solid #7ecfff;
            border-radius: 8px;
            margin: 20px 0;
            font-style: italic;
        }
        .box {
            margin: 24px 0;
            padding: 20px;
            border-radius: 8px;
            position: relative;
        }
        .definition-box {
            background: #1a2633;
            border-left: 5px solid #7ecfff;
        }
        .result-box {
            background: #23272b;
            border-left: 5px solid #a8ff78;
        }
        .code-box {
            background: #0a0a0a;
            border: 1px solid #333;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            margin: 15px 0;
            line-height: 1.4;
        }
        .label {
            font-weight: bold;
            text-transform: uppercase;
            font-size: 0.85em;
            display: block;
            margin-bottom: 8px;
            color: #7ecfff;
        }
        strong { 
            color: #7ecfff;
        }
        .meta {
            color: #999;
            font-size: 0.95em;
            margin-bottom: 20px;
        }
        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        li {
            margin: 8px 0;
        }
        a {
            color: #7ecfff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>Optimizing the computation time of the product of two "large" numbers</h1>
        
        <div class="meta">
            <strong>Author:</strong> Alexis Lecordier<br>
            <strong>Date:</strong> 30/01/26 – 06/02/26
        </div>

        <div class="abstract">
            <strong>Abstract</strong><br>
            Multiplying two numbers is an elementary task, but its computational cost becomes very high as their size increases. This article analyzes the complexity limits of naive methods, then presents a more efficient method and its improvement.
        </div>

        <h2>1. The Problem</h2>
        <p>
            Consider two real numbers $A$ and $B$ that are computer-representable, i.e., terminating decimals. There then exist two integers $a, b \in \mathbb{Z}$ and two natural numbers $e, f \in \mathbb{N}$ such that
        </p>
        <p style="text-align: center;">
            $A = a \times 10^{-e}, \quad B = b \times 10^{-f}$
        </p>
        <p>
            From this we immediately deduce that
        </p>
        <p style="text-align: center;">
            $AB = (a \times b) \times 10^{-(e+f)}$
        </p>
        <p>
            The values $a, b, e,$ and $f$ are those stored in memory. Thus, calculating $AB$ consists of performing the multiplication of two integers $a$ and $b$, then adding the exponents $e$ and $f$ to determine the final power of 10.
        </p>
        <p>
            The evaluation of the computational cost of $AB$ therefore requires studying both the complexity of an integer multiplication algorithm and that of adding two natural integers.
        </p>

        <h2>2. Elementary Addition and Multiplication Methods</h2>

        <h3>2.1 Adding Two Natural Integers</h3>
        <p>
            Let $m$ and $p$ be two natural integers written in base 10 and represented in memory as lists of digits. Let $n$ denote the maximum number of digits of $m$ and $p$. These lists are inverted to place the least significant digits first.
        </p>
        <p>
            Addition is then performed by simultaneous scanning, consisting of adding the corresponding digits term by term, while propagating successive carries.
        </p>
        <p>
            Finally, the resulting list is inverted to return to the usual decimal order, then converted back into a string representing the integer $m + p$. Since each digit is processed only once, the algorithmic complexity of addition is linear, of order $O(n)$.
        </p>
        <p>
            We introduce the notation $M_n$ to denote the algorithmic cost of multiplying two (signed) integers with at most $n$ digits in base 10. According to the previous reductions, calculating the product of two terminating decimals (of size at most $n$) reduces to one addition and one integer multiplication, giving a total cost of the form
        </p>
        <p style="text-align: center;">
            $O(n) + O(M_n)$
        </p>
        <p>
            It remains therefore to determine $M_n$ for the elementary multiplication method.
        </p>

        <h3>2.2 The "Elementary" Multiplication of Two Signed Integers</h3>
        <p>
            Let $m$ and $p$ be two signed integers. We begin by isolating the sign:
        </p>
        <p style="text-align: center;">
            $m = \sigma_m |m|, \quad p = \sigma_p |p|, \quad \sigma_m, \sigma_p \in \{-1, +1\}$
        </p>
        <p>
            The sign of the product is then $\sigma_m \sigma_p$, which is a constant-time operation $O(1)$. Thus, the study of cost reduces to the multiplication of two natural integers $|m|$ and $|p|$.
        </p>
        <p>
            We then represent $|m|$ and $|p|$ in base 10 as lists of digits, as in the previous subsection, and invert these lists to place the least significant digits first. We denote
        </p>
        <p style="text-align: center;">
            $\tilde{L}_m = [a_0, a_1, \ldots, a_{n-1}], \quad \tilde{L}_p = [b_0, b_1, \ldots, b_{n-1}]$
        </p>
        <p>
            where $n$ denotes the maximum number of digits (possibly after padding with zeros). We thus have
        </p>
        <p style="text-align: center;">
            $|m| = \sum_{i=0}^{n-1} a_i 10^i, \quad |p| = \sum_{j=0}^{n-1} b_j 10^j$
        </p>

        <h3>Multiplication as Discrete Convolution</h3>
        <p>
            By expanding, we obtain
        </p>
        <p style="text-align: center;">
            $|m||p| = \left(\sum_{i=0}^{n-1} a_i 10^i\right) \left(\sum_{j=0}^{n-1} b_j 10^j\right) = \sum_{k=0}^{2n-2} \left(\sum_{i+j=k} a_i b_j\right) 10^k$
        </p>
        <p>
            We define the <strong>discrete convolution</strong> of the sequences $a = (a_i)_{i=0}^{n-1}$ and $b = (b_j)_{j=0}^{n-1}$ by
        </p>
        <p style="text-align: center;">
            $(a * b)_k = \sum_{\substack{i+j=k \\ 0 \leq i \leq n-1 \\ 0 \leq j \leq n-1}} a_i b_j, \quad k = 0, \ldots, 2n-2$
        </p>
        <p>
            By setting $c_k = (a * b)_k$, we obtain a compact expression of the product:
        </p>
        <p style="text-align: center;">
            $|m||p| = \sum_{k=0}^{2n-2} c_k 10^k$
        </p>

        <h3>Renormalization (Carries) and Link with the "School" Method</h3>
        <p>
            The coefficients $c_k$ are not necessarily digits (they can exceed 9). We therefore perform a renormalization in base 10: we iterate through $k = 0, 1, \ldots$ while propagating carries. For each position $k$, we write
        </p>
        <p style="text-align: center;">
            $c_k + r_k = d_k + 10r_{k+1}, \quad d_k \in \{0, \ldots, 9\}$
        </p>
        <p>
            where $r_k$ is the incoming carry (with $r_0 = 0$), $d_k$ is the final digit at position $10^k$, and $r_{k+1}$ is the outgoing carry. This step formalizes exactly the multiplication learned at school: the products $a_i b_j$ are first grouped by position $10^{i+j}$ (which corresponds to the convolution), then we perform carries to obtain a valid decimal representation.
        </p>
        <p>
            After renormalization, we obtain a list of digits $(d_0, d_1, \ldots)$ such that
        </p>
        <p style="text-align: center;">
            $|m||p| = \sum_k d_k 10^k$
        </p>
        <p>
            Finally, we apply the sign $\sigma_m \sigma_p$.
        </p>

        <h3>Elementary Convolution and Complexity</h3>
        <p>
            The convolution $c = a * b$ can be computed naively using two nested loops:
        </p>
        <div class="code-box">
def convolution(L1, L2):<br>
&nbsp;&nbsp;&nbsp;&nbsp;L = [0] * (len(L1) + len(L2) - 1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;for i in range(len(L1)):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for j in range(len(L2)):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L[i+j] += L1[i] * L2[j]<br>
&nbsp;&nbsp;&nbsp;&nbsp;return L
        </div>
        <p>
            If $|L_1| \sim n$ and $|L_2| \sim n$, this computation performs on the order of $n^2$ elementary operations. Thus, for the "elementary" method, the multiplication of two $n$-digit integers satisfies
        </p>
        <p style="text-align: center;">
            $M_n = O(n^2)$
        </p>
        <p>
            At this point, the total cost of calculating the product (for terminating decimals of size at most $n$) is dominated by elementary multiplication, and thus of order $O(n^2)$. In other words, the problem essentially reduces to studying the complexity of an algorithm computing the convolution of two digit lists. In the following, we present an algorithm allowing us to replace the naive $O(n^2)$ convolution with a method in $O(n\ln n)$.
        </p>

        <h2>3. The Cooley–Tukey Fast Fourier Transform (FFT) Algorithm</h2>
        <p>
            To understand the FFT algorithm, we must first know what a discrete Fourier transform is, and link it to the computation of convolutions.
        </p>

        <h3>3.1 Definitions and Properties of the Discrete Fourier Transform</h3>
        <p>
            We work in the very general framework of (complex) sequences with finite support. Let $S_f$ denote the set of sequences $x = (x_k)_{k \in \mathbb{Z}}$ with values in $\mathbb{C}$ and with finite support.
        </p>

        <div class="definition-box">
            <span class="label">Definition 3.1</span>
            <p style="margin: 0;">
                Fix $N \in \mathbb{N}^*$. The <strong>discrete Fourier transform (DFT)</strong> of size $N$ is the linear application
            </p>
            <p style="text-align: center; margin: 10px 0 0 0;">
                $F_N : \mathbb{C}^N \rightarrow \mathbb{C}^N$
            </p>
            <p style="text-align: center; margin: 5px 0 0 0;">
                defined by
            </p>
            <p style="text-align: center; margin: 5px 0 0 0;">
                $(F_N(x))_k = \sum_{j=0}^{N-1} x_j \omega_N^{jk}, \quad k = 0, 1, \ldots, N-1$
            </p>
            <p style="margin: 5px 0 0 0;">
                If $x \in S_f$ is supported in $\{0, \ldots, N-1\}$, we then define $F_N(x)$ by identification with its vector $(x_0, \ldots, x_{N-1})$.
            </p>
        </div>

        <p>
            <strong>Remark 3.1 (Polynomial link - evaluation at roots of unity):</strong> To $x = (x_0, \ldots, x_{N-1})$, associate the polynomial
        </p>
        <p style="text-align: center;">
            $P_x(X) = \sum_{j=0}^{N-1} x_j X^j$
        </p>
        <p>
            Then the previous definition simply writes:
        </p>
        <p style="text-align: center;">
            $(F_N(x))_k = P_x(\omega_N^k)$
        </p>
        <p>
            Thus, computing $F_N(x)$ amounts to evaluating $P_x$ at the $N$ points $\omega_N^0, \ldots, \omega_N^{N-1}$.
        </p>

        <div class="definition-box">
            <span class="label">Proposition 3.1</span>
            <p style="margin: 0;">
                <strong>Proposition (convolution by DFT after "zero-padding"):</strong> Let $a, b \in S_f$ be two sequences with finite support, and $c = a * b$ their convolution. Suppose that $\text{supp}(a) \subset \{0, \ldots, n-1\}$ and $\text{supp}(b) \subset \{0, \ldots, m-1\}$, so that $\text{supp}(c) \subset \{0, \ldots, n+m-2\}$. Fix an integer $N \geq n+m-1$, and consider the zero-padding extensions
            </p>
            <p style="text-align: center; margin: 10px 0 0 0;">
                $a^{(N)} = (a_0, \ldots, a_{N-1}) \in \mathbb{C}^N, \quad b^{(N)} = (b_0, \ldots, b_{N-1}) \in \mathbb{C}^N$
            </p>
            <p style="margin: 5px 0 0 0;">
                (where $a_k = 0$ for $k \geq n$ and $b_k = 0$ for $k \geq m$). Then, by noting $c^{(N)} \in \mathbb{C}^N$ the zero-padding extension of $c$, we have the relation
            </p>
            <p style="text-align: center; margin: 10px 0 0 0;">
                $F_N(c^{(N)}) = F_N(a^{(N)}) \odot F_N(b^{(N)})$
            </p>
            <p style="margin: 5px 0 0 0;">
                where $\odot$ denotes the pointwise product.
            </p>
        </div>

        <p>
            <strong>Proof (Polynomial argument):</strong> We associate to $a^{(N)}$ and $b^{(N)}$ the polynomials
        </p>
        <p style="text-align: center;">
            $P_a(X) = \sum_{i=0}^{N-1} a_i X^i, \quad P_b(X) = \sum_{j=0}^{N-1} b_j X^j$
        </p>
        <p>
            By expanding,
        </p>
        <p style="text-align: center;">
            $P_a(X)P_b(X) = \sum_{k=0}^{2N-2} \left(\sum_{i+j=k} a_i b_j\right) X^k$
        </p>
        <p>
            For $k \leq n+m-2$, terms with $i \geq n$ or $j \geq m$ are zero, so the coefficient of $X^k$ is
        </p>
        <p style="text-align: center;">
            $\sum_{i+j=k} a_i b_j = c_k$
        </p>
        <p>
            Thus, by setting $P_c(X) = \sum_{k=0}^{n+m-2} c_k X^k$, we have indeed
        </p>
        <p style="text-align: center;">
            $P_c(X) = P_a(X)P_b(X)$ (at the level of coefficients up to degree $n+m-2$)
        </p>
        <p>
            By evaluating at $X = \omega_N^r$ ($\omega_N = e^{-2\pi i / N}$), for all $r = 0, \ldots, N-1$, we obtain
        </p>
        <p style="text-align: center;">
            $P_c(\omega_N^r) = P_a(\omega_N^r)P_b(\omega_N^r)$
        </p>
        <p>
            Now $(F_N(a^{(N)}))_r = P_a(\omega_N^r)$, $(F_N(b^{(N)}))_r = P_b(\omega_N^r)$ and $(F_N(c^{(N)}))_r = P_c(\omega_N^r)$, from which
        </p>
        <p style="text-align: center;">
            $(F_N(c^{(N)}))_r = (F_N(a^{(N)}))_r (F_N(b^{(N)}))_r$
        </p>
        <p>
            which is equivalent to
        </p>
        <p style="text-align: center;">
            $F_N(c^{(N)}) = F_N(a^{(N)}) \odot F_N(b^{(N)})$ ∎
        </p>

        <p>
            <strong>Inverse Transform:</strong> The application $F_N$ is invertible, and its inverse is naturally given by:
        </p>
        <p style="text-align: center;">
            $(F_N^{-1}(X))_j = \frac{1}{N} \sum_{k=0}^{N-1} X_k \omega_N^{-jk}, \quad j = 0, 1, \ldots, N-1$
        </p>

        <p>
            From this we deduce the calculation formula:
        </p>
        <p style="text-align: center;">
            $(a * b)^{(N)} = F_N^{-1}(F_N(a^{(N)}) \odot F_N(b^{(N)}))$
        </p>

        <p>
            We note here that to obtain the convolution of $a$ and $b$, it suffices to:
        </p>
        <ol>
            <li>compute $F_N(a^{(N)})$ and $F_N(b^{(N)})$;</li>
            <li>perform their <strong>pointwise product</strong> (of complexity $O(N)$);</li>
            <li>apply the inverse transform $F_N^{-1}$.</li>
        </ol>

        <p>
            Thus, if we have an algorithm of complexity $O(N \ln N)$ for computing $F_N(s)$ and $F_N^{-1}(s)$ (where $s$ is a finite-support sequence of size $N$), then the convolution computation satisfies a complexity of:
        </p>
        <p style="text-align: center;">
            $\underbrace{O(N\ln N)}_{\text{$F_N(a^{(N)})$}} + \underbrace{O(N\ln N)}_{\text{$F_N(b^{(N)})$}} + \underbrace{O(N)}_{\text{pointwise product}} + \underbrace{O(N\ln N)}_{\text{$F_N^{-1}$}} = O(N\ln N)$
        </p>

        <h3>3.2 Principle of the Algorithm</h3>
        <p>
            Let $a = (a_0, \ldots, a_{N-1}) \in \mathbb{C}^N$. According to Remark 3.1, computing the discrete Fourier transform
        </p>
        <p style="text-align: center;">
            $F_N(a) = \left((F_N(a))_k\right)_{0 \leq k \leq N-1}$
        </p>
        <p>
            amounts to evaluating the polynomial
        </p>
        <p style="text-align: center;">
            $P_a(X) = \sum_{j=0}^{N-1} a_j X^j$
        </p>
        <p>
            at the $N$ $N$-th roots of unity:
        </p>
        <p style="text-align: center;">
            $(F_N(a))_k = P_a(\omega_N^k), \quad k = 0, \ldots, N-1, \quad \text{where } \omega_N = e^{-2\pi i / N}$
        </p>

        <p>
            A naive evaluation of $P_a$ at one point costs $O(N)$, so evaluation at $N$ points costs $O(N^2)$. The idea of Cooley–Tukey is to exploit a recursive structure when $N$ is a power of 2. We thus assume $N = 2^s$ (if necessary, we pad the sequence $a$ with zeros up to the next power of 2, which amounts to completing $P_a$ with zero coefficients).
        </p>

        <p>
            We then write the even/odd decomposition of $P_a$:
        </p>
        <p style="text-align: center;">
            $P_a(X) = P_{\text{even}}(X^2) + X P_{\text{odd}}(X^2)$
        </p>
        <p>
            where
        </p>
        <p style="text-align: center;">
            $P_{\text{even}}(Y) = \sum_{r=0}^{N/2-1} a_{2r} Y^r, \quad P_{\text{odd}}(Y) = \sum_{r=0}^{N/2-1} a_{2r+1} Y^r$
        </p>

        <p>
            We want to evaluate $P_a$ at $X = \omega_N^k$. Now $(\omega_N^k)^2 = \omega_{N/2}^k$, so
        </p>
        <p style="text-align: center;">
            $P_a(\omega_N^k) = P_{\text{even}}(\omega_{N/2}^k) + \omega_N^k P_{\text{odd}}(\omega_{N/2}^k)$
        </p>

        <p>
            Thus, evaluation of $P_a$ at the $N$ $N$-th roots reduces to evaluation of two polynomials $P_{\text{even}}$ and $P_{\text{odd}}$ at the $N/2$ $(N/2)$-th roots.
        </p>

        <p>
            Moreover, for any $d$-th root of unity $\omega_d$, we have the relation
        </p>
        <p style="text-align: center;">
            $\omega_d^{i+d/2} = -\omega_d^i, \quad i = 0, \ldots, d/2-1$
        </p>

        <p>
            In particular, by setting $d = N$, we obtain for $k = 0, \ldots, N/2-1$:
        </p>
        <p style="text-align: center;">
            $\omega_N^{k+N/2} = -\omega_N^k$
        </p>

        <p>
            We can then simultaneously calculate the two values
        </p>
        <p style="text-align: center;">
            $P_a(\omega_N^k) \text{ and } P_a(\omega_N^{k+N/2})$
        </p>
        <p>
            from the same quantities $P_{\text{even}}(\omega_{N/2}^k)$ and $P_{\text{odd}}(\omega_{N/2}^k)$:
        </p>
        <p style="text-align: center;">
            $P_a(\omega_N^k) = P_{\text{even}}(\omega_{N/2}^k) + \omega_N^k P_{\text{odd}}(\omega_{N/2}^k)$<br>
            $P_a(\omega_N^{k+N/2}) = P_{\text{even}}(\omega_{N/2}^k) - \omega_N^k P_{\text{odd}}(\omega_{N/2}^k)$
        </p>

        <p>
            We then repeat exactly the same procedure on $P_{\text{even}}$ and $P_{\text{odd}}$ (by again separating their even/odd coefficients), which produces a recursive decomposition until reaching degree 0 polynomials (trivial evaluation). This recursion corresponds to a tree whose level $t$ manipulates evaluations at the $2^{s-t}$-th roots of unity.
        </p>

        <div style="text-align: center; margin: 20px 0;">
            <p><strong>Figure 1 – Tree of reduction of roots of unity for $N = 8$ (descent by the mapping $x \mapsto x^2$)</strong></p>
            <p style="font-family: 'Courier New', monospace; font-size: 0.85em;">
                ω₀₈  ω₁₈  ω₂₈  ω₃₈  ω₄₈  ω₅₈  ω₆₈  ω₇₈<br>
                &nbsp;&nbsp;&nbsp;ω₀₄  ω₁₄  ω₂₄  ω₃₄<br>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ω₀₂  ω₁₂<br>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ω₀₁ = 1
            </p>
        </div>

        <p>
            <strong>Algorithm (Python):</strong>
        </p>
        <div class="code-box">
def FFT(L):<br>
&nbsp;&nbsp;&nbsp;&nbsp;n = len(L)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;if n == 1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return L<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;w = np.exp(2j * np.pi / n)<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;L_even = L[0::2]<br>
&nbsp;&nbsp;&nbsp;&nbsp;L_odd = L[1::2]<br>
&nbsp;&nbsp;&nbsp;&nbsp;y_even, y_odd = FFT(L_even), FFT(L_odd)<br>
&nbsp;&nbsp;&nbsp;&nbsp;y = [0] * n<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;for i in range(n // 2):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i] = y_even[i] + (w ** i) * y_odd[i]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y[i + n // 2] = y_even[i] - (w ** i) * y_odd[i]<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;return y
        </div>

        <h3>3.3 Complexity Calculation (Counting by Levels)</h3>
        <p>
            Suppose $n = 2^s$ (after zero-padding if necessary). The recursion tree then has
        </p>
        <p style="text-align: center;">
            $s = \left\lfloor \frac{\ln(n)}{\ln(2)} \right\rfloor \sim \ln(n)$
        </p>
        <p>
            levels.
        </p>

        <p>
            <strong>Descent (even/odd separation):</strong> At level $k$ (with $1 \leq k \leq s$), we manipulate sublists of size $n/2^k$. The total cost of the descent is bounded by
        </p>
        <p style="text-align: center;">
            $\sum_{k=1}^{s} \left(\frac{n}{2^k} + 1\right)$
        </p>
        <p>
            where $n/2^k$ corresponds to the work of separation (construction of even/odd sublists), and $1$ to the calculation of the root $w$ associated with the considered level.
        </p>

        <p>
            <strong>Ascent:</strong> The ascent traverses the same $s$ levels. At level $k$, we reconstruct a list of size $n/2^{s-k}$. We thus perform $n/2^{s-k}$ butterfly-type combinations, each consisting of two lines:
        </p>
        <p style="text-align: center;">
            $y[i] = y_{\text{even}}[i] + (w^i)y_{\text{odd}}[i]$<br>
            $y[i+n/2] = y_{\text{even}}[i] - (w^i)y_{\text{odd}}[i]$
        </p>

        <p>
            For one line, the term $1$ corresponds to addition/subtraction, while the term $\text{cplx}((w^i)y_{\text{odd}}[i])$ corresponds to the product calculation. Thus, the cost of the ascent is bounded by
        </p>
        <p style="text-align: center;">
            $\sum_{k=1}^{s} \left(\frac{n}{2^{s-k}} \cdot 2(1 + \text{cplx}((w^i)y_{\text{odd}}[i]))\right)$
        </p>

        <p>
            <strong>Cost of $(w^i)y_{\text{odd}}[i]$ and Implementation Choice:</strong> In our case, we have already calculated and stored the root $w$ associated with the considered size. Consequently, the calculation of the term $(w^i)y_{\text{odd}}[i]$ reduces to an exponentiation of $w$ (which is an addition of arguments) and a multiplication by an integer ($y_{\text{odd}}[i]$), so its complexity is of order
        </p>
        <p style="text-align: center;">
            $\text{cplx}((w^i)y_{\text{odd}}[i]) = O(1)$
        </p>

        <p>
            <strong>Summary:</strong> By combining descent and ascent, we obtain the bound
        </p>
        <p style="text-align: center;">
            $\sum_{k=1}^{s} \left(\frac{n}{2^k} + 1\right) + \sum_{k=1}^{s} \left(\frac{n}{2^{s-k}} \cdot 2(1 + O(1))\right)$
        </p>
        <p>
            with $s = \ln(n)$. With s going to infinity we have:
        </p>
        <p style="text-align: center;">
            $O(n \log_2 n)$
        </p>

        <p>
            We thus obtain that the computation of the convolution of $a$ and $b$ via FFT decomposes as:
        </p>
        <p style="text-align: center;">
            $\underbrace{O(n\ln n)}_{\text{FFT of $a$}} + \underbrace{O(n\ln n)}_{\text{FFT of $b$}} + \underbrace{O(n)}_{\text{pointwise product}} + \underbrace{O(n\ln n)}_{\text{Inverse FFT}} = O(n\ln n)$
        </p>

        <h2>Conclusion</h2>

        <div style="text-align:center; margin:20px 0;">
            <img src="images/fft_graph.png" alt="Comparative timing: FFT vs naive" style="max-width:100%; height:auto; border:1px solid #333; border-radius:6px;" />
            <p style="color:#999; margin-top:8px;">Figure 2 – Execution time: FFT (orange) vs naive method (blue). Note: the graph labels are in French.</p>
            <p style="color:#999; margin-top:6px;">The FFT algorithm is implemented in libraries such as NumPy, which provide variants and optimizations (real-valued computation, advanced numerical techniques) to reduce rounding errors on the real parts of complex numbers during transforms.</p>
        </div>

        
    </div>
</body>
</html>